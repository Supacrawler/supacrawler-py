{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting supacrawler-py\n",
            "  Downloading supacrawler_py-0.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting httpx>=0.27.0 (from supacrawler-py)\n",
            "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting pydantic>=2.6.0 (from supacrawler-py)\n",
            "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
            "Collecting anyio (from httpx>=0.27.0->supacrawler-py)\n",
            "  Downloading anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting certifi (from httpx>=0.27.0->supacrawler-py)\n",
            "  Downloading certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting httpcore==1.* (from httpx>=0.27.0->supacrawler-py)\n",
            "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting idna (from httpx>=0.27.0->supacrawler-py)\n",
            "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.27.0->supacrawler-py)\n",
            "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic>=2.6.0->supacrawler-py)\n",
            "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.33.2 (from pydantic>=2.6.0->supacrawler-py)\n",
            "  Downloading pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
            "Collecting typing-extensions>=4.12.2 (from pydantic>=2.6.0->supacrawler-py)\n",
            "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting typing-inspection>=0.4.0 (from pydantic>=2.6.0->supacrawler-py)\n",
            "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting sniffio>=1.1 (from anyio->httpx>=0.27.0->supacrawler-py)\n",
            "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Downloading supacrawler_py-0.1.0-py3-none-any.whl (5.5 kB)\n",
            "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
            "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
            "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
            "Downloading pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
            "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
            "Downloading anyio-4.10.0-py3-none-any.whl (107 kB)\n",
            "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
            "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
            "Installing collected packages: typing-extensions, sniffio, idna, h11, certifi, annotated-types, typing-inspection, pydantic-core, httpcore, anyio, pydantic, httpx, supacrawler-py\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/13\u001b[0m [supacrawler-py]m [pydantic]\n",
            "\u001b[1A\u001b[2KSuccessfully installed annotated-types-0.7.0 anyio-4.10.0 certifi-2025.8.3 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 pydantic-2.11.7 pydantic-core-2.33.2 sniffio-1.3.1 supacrawler-py-0.1.0 typing-extensions-4.14.1 typing-inspection-0.4.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install supacrawler-py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First let's start with a basic scrape:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sk_BOspqBDk2JAw50zpfpVaPEr51vFebs0u\n",
            "success=True url='https://supacrawler.com' content=\"[**We're in beta!** Get **5000 credits** for free when you subscribe to any plan. _Join the beta now_ →](/signup)\\n\\n# Scrape Everything\\n\\n[Get 500 credits for free](/signup) [Get Started](/dashboard)\\n\\n## Deploy faster\\n\\nBuilt for your needs\\n\\nScrape, transform, and load data from any website.\\n\\nCustomize your data to fit your needs.\\n\\n## Developer-first\\n\\nThree powerful APIs\\n\\nExtract data from any website with our simple, reliable APIs. From single pages to entire websites, we handle the complexity so you can focus on building.\\n\\nScrape APIExtract clean, structured content from any webpage. Get markdown, HTML, or plain text with automatic JavaScript rendering.\\n\\nJobs APICrawl entire websites asynchronously. Scale from single pages to thousands with zero infrastructure management.\\n\\nScreenshots APICapture pixel-perfect screenshots of any webpage with customizable viewport and device settings.\\n\\n```\\n\\n1// Scrape API - Extract clean content from any webpage\\n2const response = await fetch('https://api.supacrawler.com/api/v1/scrape', {\\n3 method: 'GET',\\n4 headers: {\\n5 'Authorization': 'Bearer YOUR_API_KEY',\\n6 },\\n7 body: new URLSearchParams({\\n8 url: 'https://example.com',\\n9 format: 'markdown',\\n10 }),\\n11});\\n12\\n\\n13const data = await response.json();\\n14console.log(data.content); // Clean markdown content\\n```\\n\\n![Logo](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Flogo.90b998c8.jpg&w=3840&q=75)\\nSupacrawler\\n\\n© Manila Research LLC. 2025\" title='Supacrawler' metadata={'status_code': 200}\n"
          ]
        }
      ],
      "source": [
        "# Supacrawler Python SDK - Scrape Examples\n",
        "from supacrawler import SupacrawlerClient, ScrapeParams\n",
        "import os\n",
        "\n",
        "SUPACRAWLER_API_KEY=os.environ.get(\"SUPACRAWLER_API_KEY\", \"YOUR_API_KEY\")\n",
        "print(SUPACRAWLER_API_KEY)\n",
        "\n",
        "client = SupacrawlerClient(api_key=SUPACRAWLER_API_KEY)\n",
        "\n",
        "# Basic markdown scrape\n",
        "res_md = client.scrape(ScrapeParams(url=\"https://supacrawler.com\", format=\"markdown\"))\n",
        "print(res_md)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A good example of a website that won't work unless you set `render_js=True` is `https://ai.google.dev/gemini-api/docs`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This will not work for google with the following error:\n",
            " HTTP 500: {'success': False, 'url': 'https://ai.google.dev/gemini-api/docs', 'metadata': {'status_code': 500}}\n",
            "This will work:\n",
            " success=True url='https://supacrawler.com' content='[**We\\'re in beta!** Get **5000 credits** for free when you subscribe to any plan. _Join the beta now_ →](/signup)\\n\\nDismiss\\n\\n[![Logo](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Flogo.90b998c8.jpg&w=3840&q=75)\\\\\\nSupacrawler](/)\\n\\n[Products](/dashboard/scrape) [Docs](https://docs.supacrawler.com) [Blog](/blog) [Pricing](/pricing) [FAQ](/pricing#faq)\\n\\n[Sign In](/signin)\\n\\n# Scrape Everything\\n\\n## One tool to extract from any site, any format\\n\\n### Try it for free. No credit card required.\\n\\n[Get 500 credits for free](/signup) [Get Started](/dashboard)\\n\\nScraper\\n\\nCrawler\\n\\nScreenshots\\n\\n![Scraper](/_next/image?url=%2Fscreenshots%2Fscraper.webp&w=3840&q=75)\\n\\n## Deploy faster\\n\\nBuilt for your needs\\n\\nScrape, transform, and load data from any website.\\n\\nCustomize your data to fit your needs.\\n\\n[https://ai.google.dev/gemini-api/docs](https://ai.google.dev/gemini-api/docs)\\n\\nJSON\\n\\nMarkdown\\n\\nCopy\\n\\n````\\n{\\n\"url\": \"https://ai.google.dev/gemini-api/docs\",\\n\"content\": \"# Gemini Developer API\\\\n\\\\n[Get a Gemini API Key](https://aistudio.google.com/apikey)\\\\n\\\\nGet a Gemini API key and make your first API request in minutes.\\\\n\\\\n## Python\\\\n```\\\\nfrom google import genai\\\\n\\\\nclient = genai.Client(api_key=\\\\\"YOUR_API_KEY\\\\\")\\\\n\\\\nresponse = client.models.generate_content(\\\\n model=\\\\\"gemini-2.0-flash\\\\\",\\\\n contents=\\\\\"Explain how AI works\\\\\",\\\\n)\\\\n\\\\nprint(response.text)\\\\n```\",\\n\"title\": \"Gemini API | Google AI for Developers\",\\n\"metadata\": {\\n\"title\": \"Gemini API | Google AI for Developers\",\\n\"description\": \"Gemini Developer API Docs and API Reference\",\\n\"ogTitle\": \"Gemini API | Google AI for Developers\",\\n\"ogImage\": \"https://ai.google.dev/static/site-assets/images/share-gemini-api.png\",\\n\"ogSiteName\": \"Google AI for Developers\",\\n\"language\": \"en\"\\n}\\n}\\n````\\n\\nJSON extracted successfully\\n\\n0.8KB\\n\\n### Instant Extraction\\n\\nOne-Click Website Scraping\\n\\nExtract data from complex sites with perfect structure detection. Export as JSON/MD instantly.\\n\\nArticle\\n\\nTable\\n\\nProducts\\n\\n### Smart Parsing\\n\\nAuto-Format Recognition\\n\\nAI-powered detection for articles, product listings, and tables with suggested CSS selectors\\n\\nhttps://shop.products.com\\n\\n$10.99\\n\\n$20.99\\n\\n### Headless Browser\\n\\nScrape Dynamic Websites\\n\\nAutomatically render and scrape dynamic content from modern SPAs and PWAs using headless Chrome\\n\\n![Logo](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Flogo.90b998c8.jpg&w=3840&q=75)\\n\\n![](/logo-cluster/career-builder.svg)![](/logo-cluster/dribbble.svg)![](/logo-cluster/glassdoor.svg)![](/logo-cluster/linkedin.svg)![](/logo-cluster/upwork.svg)![](/logo-cluster/we-work-remotely.svg)\\n\\n### Platform Coverage\\n\\nScrape Major Platforms\\n\\nExtract data from major platforms with unified API access.\\n\\nIP Rotation\\n\\nActive\\n\\nCurrent Session\\n\\nConnected\\n\\nProxy Server 1\\n\\nActive\\n\\nIP: 192.168.1.20Location: US\\n\\nProxy Server 2\\n\\nStandby\\n\\nIP: 192.168.2.30Location: EU\\n\\n### Anti-Block\\n\\nStealth Crawling\\n\\nAutomatic IP rotation and request throttling to avoid bot detection\\n\\n## Developer-first\\n\\nThree powerful APIs\\n\\nExtract data from any website with our simple, reliable APIs. From single pages to entire websites, we handle the complexity so you can focus on building.\\n\\nScrape APIExtract clean, structured content from any webpage. Get markdown, HTML, or plain text with automatic JavaScript rendering.\\n\\nJobs APICrawl entire websites asynchronously. Scale from single pages to thousands with zero infrastructure management.\\n\\nScreenshots APICapture pixel-perfect screenshots of any webpage with customizable viewport and device settings.\\n\\ncURLTypeScriptPython\\n\\nScrapeJobsScreenshots\\n\\n```\\n\\n1// Scrape API - Extract clean content from any webpage\\n2const response = await fetch(\\'https://api.supacrawler.com/api/v1/scrape\\', {\\n3 method: \\'GET\\',\\n4 headers: {\\n5 \\'Authorization\\': \\'Bearer YOUR_API_KEY\\',\\n6 },\\n7 body: new URLSearchParams({\\n8 url: \\'https://example.com\\',\\n9 format: \\'markdown\\',\\n10 }),\\n11});\\n12\\n\\n13const data = await response.json();\\n14console.log(data.content); // Clean markdown content\\n```\\n\\n- Product\\n\\n- [Web Scraping](https://docs.supacrawler.com/api/scrape)\\n- [Web Crawling](https://docs.supacrawler.com/api/jobs)\\n- [Screenshot](https://docs.supacrawler.com/api/screenshots)\\n- [See all →](/work)\\n- Company\\n\\n- [About](/about)\\n- [Blog](/blog)\\n- [Privacy Policy](/privacy-policy)\\n- [Terms of Service](/terms-of-service)\\n- [Contact us](/contact)\\n- Blog\\n\\n- [Latest](/blog)\\n- [The Foundation](/blog/the-foundation)\\n\\n## Sign up for our newsletter\\n\\nSubscribe to get the latest design news, articles, resources and inspiration.\\n\\n![Logo](/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Flogo.90b998c8.jpg&w=3840&q=75)\\nSupacrawler\\n\\n© Manila Research LLC. 2025' title='Supacrawler' metadata={'status_code': 200}\n"
          ]
        }
      ],
      "source": [
        "# This happens because google redirects you to authenticate with google account. Catch the error and log it.\n",
        "try:\n",
        "    res_md = client.scrape(ScrapeParams(url=\"https://ai.google.dev/gemini-api/docs\", format=\"markdown\"))\n",
        "except Exception as e:\n",
        "    print(\"This will not work for google with the following error:\\n\", e)\n",
        "\n",
        "# Rendering JS should work!\n",
        "res_md_rendered = client.scrape(ScrapeParams(url=\"https://supacrawler.com\", format=\"markdown\", render_js=True))\n",
        "print(\"This will work:\\n\", res_md_rendered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also extract all the links within a starting url:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "success=True url='https://supacrawler.com' links=['https://supacrawler.com/about', 'https://supacrawler.com/signup', 'https://supacrawler.com/', 'https://supacrawler.com/signin', 'https://supacrawler.com/work', 'https://supacrawler.com/dashboard/scrape', 'https://supacrawler.com/dashboard', 'https://supacrawler.com/blog', 'https://supacrawler.com/pricing'] discovered=9 metadata={'status_code': 200, 'depth': 2}\n"
          ]
        }
      ],
      "source": [
        "# Links mapping with depth and max_links\n",
        "res_links = client.scrape(ScrapeParams(url=\"https://supacrawler.com\", format=\"links\", depth=2, max_links=10))\n",
        "print(res_links)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
