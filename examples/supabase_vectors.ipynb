{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Supabase Vectors with Supacrawler\n",
        "This notebook scrapes content with `Supacrawler`, embeds it, and stores vectors in Supabase (pgvector).\n",
        "\n",
        "Ensure pgvector is enabled before running:\n",
        "https://supabase.com/docs/guides/database/extensions/pgvector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.55.0-py3-none-any.whl.metadata (39 kB)\n",
            "Collecting torch\n",
            "  Downloading torch-2.8.0-cp313-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
            "Requirement already satisfied: filelock in /Users/antoineross/Desktop/Startups/Supacrawler/.venv/lib/python3.13/site-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/antoineross/Desktop/Startups/Supacrawler/.venv/lib/python3.13/site-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/antoineross/Desktop/Startups/Supacrawler/.venv/lib/python3.13/site-packages (from transformers) (2.3.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/antoineross/Desktop/Startups/Supacrawler/.venv/lib/python3.13/site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/antoineross/Desktop/Startups/Supacrawler/.venv/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/antoineross/Desktop/Startups/Supacrawler/.venv/lib/python3.13/site-packages (from transformers) (2025.7.34)\n",
            "Requirement already satisfied: requests in /Users/antoineross/Desktop/Startups/Supacrawler/.venv/lib/python3.13/site-packages (from transformers) (2.32.4)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
            "  Downloading tokenizers-0.21.4-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers)\n",
            "  Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/antoineross/Desktop/Startups/Supacrawler/.venv/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/antoineross/Desktop/Startups/Supacrawler/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/antoineross/Desktop/Startups/Supacrawler/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/antoineross/Desktop/Startups/Supacrawler/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: setuptools in /Users/antoineross/Desktop/Startups/Supacrawler/.venv/lib/python3.13/site-packages (from torch) (80.9.0)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx in /Users/antoineross/Desktop/Startups/Supacrawler/.venv/lib/python3.13/site-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /Users/antoineross/Desktop/Startups/Supacrawler/.venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/antoineross/Desktop/Startups/Supacrawler/.venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/antoineross/Desktop/Startups/Supacrawler/.venv/lib/python3.13/site-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/antoineross/Desktop/Startups/Supacrawler/.venv/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/antoineross/Desktop/Startups/Supacrawler/.venv/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/antoineross/Desktop/Startups/Supacrawler/.venv/lib/python3.13/site-packages (from requests->transformers) (2025.8.3)\n",
            "Downloading transformers-4.55.0-py3-none-any.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.8.0-cp313-none-macosx_11_0_arm64.whl (73.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl (432 kB)\n",
            "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mpmath, sympy, safetensors, torch, tokenizers, transformers\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [transformers][0m [transformers]\n",
            "\u001b[1A\u001b[2KSuccessfully installed mpmath-1.3.0 safetensors-0.6.2 sympy-1.14.0 tokenizers-0.21.4 torch-2.8.0 transformers-4.55.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU vecs datasets llama_index html2text\n",
        "%pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Supabase Vectors with Supacrawler (Vecs)\n",
        "import os\n",
        "import vecs  # Supabase Python client for vectors\n",
        "from supacrawler import SupacrawlerClient, ScrapeParams\n",
        "\n",
        "# Optional: local embeddings (Hugging Face) or hosted (OpenAI)\n",
        "USE_HF = True\n",
        "HF_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'  # 384 dims\n",
        "\n",
        "SUPACRAWLER_API_KEY = os.environ.get('SUPACRAWLER_API_KEY', 'YOUR_API_KEY')\n",
        "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY', 'YOUR_OPENAI_KEY') # Optional\n",
        "DATABASE_URL = os.environ.get('DATABASE_URL', 'postgresql://postgres:postgres@127.0.0.1:64322/postgres?sslmode=disable')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "crawler = SupacrawlerClient(api_key=SUPACRAWLER_API_KEY)\n",
        "scrape = crawler.scrape(ScrapeParams(url='https://docs.supacrawler.com/api/install', format='markdown'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prepared 1 chunks (dim=384)\n"
          ]
        }
      ],
      "source": [
        "# Chunk + embed utilities\n",
        "if USE_HF:\n",
        "    from transformers import AutoTokenizer, AutoModel\n",
        "    import torch\n",
        "    tokenizer = AutoTokenizer.from_pretrained(HF_MODEL)\n",
        "    model = AutoModel.from_pretrained(HF_MODEL)\n",
        "    def embed_text(text: str):\n",
        "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        # Mean pooling over token embeddings\n",
        "        vec = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy().tolist()\n",
        "        return vec\n",
        "    def split_text_tokens(text: str, max_tokens: int = 300, overlap: int = 50):\n",
        "        ids = tokenizer(text, return_tensors=None, add_special_tokens=False)['input_ids']\n",
        "        chunks = []\n",
        "        step = max_tokens - overlap\n",
        "        for i in range(0, len(ids), step):\n",
        "            window = ids[i:i+max_tokens]\n",
        "            chunk_txt = tokenizer.decode(window, skip_special_tokens=True)\n",
        "            chunks.append(chunk_txt)\n",
        "        return chunks if chunks else [text]\n",
        "else:\n",
        "    from openai import OpenAI\n",
        "    openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "    def embed_text(text: str):\n",
        "        resp = openai_client.embeddings.create(model='text-embedding-3-small', input=text)\n",
        "        return resp.data[0].embedding\n",
        "    def split_text_chars(text: str, max_chars: int = 1200, overlap_chars: int = 200):\n",
        "        chunks = []\n",
        "        step = max_chars - overlap_chars\n",
        "        for i in range(0, len(text), step):\n",
        "            chunks.append(text[i:i+max_chars])\n",
        "        return chunks if chunks else [text]\n",
        "\n",
        "# Create chunks based on the active embedder\n",
        "chunks = split_text_tokens(scrape.content) if USE_HF else split_text_chars(scrape.content)\n",
        "num_chunks = len(chunks)\n",
        "# Probe dim from first chunk\n",
        "first_vec = embed_text(chunks[0])\n",
        "vector_dim = len(first_vec)\n",
        "print(f\"Prepared {num_chunks} chunks (dim={vector_dim})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upserted 1 chunks (dim=384)\n"
          ]
        }
      ],
      "source": [
        "# Upsert via Vecs (all chunks)\n",
        "vx = vecs.create_client(DATABASE_URL)\n",
        "col = vx.get_or_create_collection(name='documents', dimension=vector_dim)\n",
        "records = []\n",
        "for idx, chunk in enumerate(chunks):\n",
        "    vec = first_vec if idx == 0 else embed_text(chunk)\n",
        "    rec_id = f\"{scrape.url}#chunk-{idx}\"\n",
        "    records.append((rec_id, vec, {\n",
        "        'url': scrape.url,\n",
        "        'title': getattr(scrape, 'title', None),\n",
        "        'chunk_index': idx,\n",
        "        'content': chunk,\n",
        "    }))\n",
        "col.upsert(records=records)\n",
        "print(f\"Upserted {len(records)} chunks (dim={vector_dim})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: What does supacrawler do?\n",
            "Matches: 1\n",
            "('https://docs.supacrawler.com/api/install#chunk-0', {'url': 'https://docs.supacrawler.com/api/install', 'title': 'Installation - Supacrawler API Reference', 'content': '# installation use the official s ... (27 characters truncated) ... experience with supacrawler. # # [ javascript / typescript ] ( \\\\ # java - script - type - script ) # # [ python ] ( \\\\ # python )', 'chunk_index': 0}) n/a None chunk=None\n"
          ]
        }
      ],
      "source": [
        "# Test query and log results\n",
        "from vecs import IndexMeasure\n",
        "query = \"What does supacrawler do?\"\n",
        "qvec = embed_text(query)\n",
        "\n",
        "# Utility: normalize vecs query results into (id, score, metadata)\n",
        "def normalize_vecs_result(rec):\n",
        "    if isinstance(rec, tuple):\n",
        "        if len(rec) == 4:\n",
        "            rec_id, score, _vec, metadata = rec\n",
        "        elif len(rec) == 3:\n",
        "            rec_id, score, metadata = rec\n",
        "        else:\n",
        "            return str(rec), None, {}\n",
        "        return rec_id, score, (metadata or {})\n",
        "    if isinstance(rec, dict):\n",
        "        return rec.get('id'), rec.get('score'), rec.get('metadata', {})\n",
        "    return str(rec), None, {}\n",
        "\n",
        "# Ensure the index exists\n",
        "try:\n",
        "    col.create_index(measure=IndexMeasure.cosine_distance)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "matches = col.query(data=qvec, limit=1, include_metadata=True)\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"Matches: {len(matches)}\")\n",
        "for rec in matches:\n",
        "    rec_id, score, metadata = normalize_vecs_result(rec)\n",
        "    title = metadata.get('title') if isinstance(metadata, dict) else None\n",
        "    chunk_idx = metadata.get('chunk_index') if isinstance(metadata, dict) else None\n",
        "    snippet = (metadata.get('content') or '')[:240] if isinstance(metadata, dict) else ''\n",
        "    print(rec_id, (round(score, 4) if isinstance(score, (int, float)) else 'n/a'), title, f\"chunk={chunk_idx}\")\n",
        "    if snippet:\n",
        "        print('  ', snippet.replace('\\n', ' ') + ('...' if len(snippet) == 240 else ''))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crawl + embed docs, then ask a question\n",
        "from supacrawler import JobCreateRequest\n",
        "\n",
        "crawler = SupacrawlerClient(api_key=SUPACRAWLER_API_KEY)\n",
        "\n",
        "# 1) Start a small crawl (scoped)\n",
        "job = crawler.create_job(JobCreateRequest(\n",
        "    url='https://docs.supacrawler.com',\n",
        "    type='crawl',\n",
        "    depth=1,\n",
        "    link_limit=10,\n",
        "    render_js=False,\n",
        "))\n",
        "status = crawler.wait_for_job(job.job_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "completed\n",
            "Pages: 8\n"
          ]
        }
      ],
      "source": [
        "# Poll until completion\n",
        "final = crawler.wait_for_job(job.job_id, interval_seconds=3.0, timeout_seconds=60.0)\n",
        "print(final.status)\n",
        "if final.status == \"completed\" and final.data is not None:\n",
        "    if hasattr(final.data, \"crawl_data\"):\n",
        "        print(\"Pages:\", len(final.data.crawl_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1378 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upserted crawl chunks: 19\n"
          ]
        }
      ],
      "source": [
        "# 2) Upsert each page (HuggingFace by default here)\n",
        "site_records = []\n",
        "for page_url, page in (status.data.crawl_data or {}).items():\n",
        "    content = (page.markdown or '')\n",
        "    if not content:\n",
        "        continue\n",
        "    # Chunk and embed\n",
        "    page_chunks = split_text_tokens(content) if USE_HF else split_text_chars(content)\n",
        "    for idx, chunk in enumerate(page_chunks):\n",
        "        vec = embed_text(chunk)\n",
        "        site_records.append((f\"{page_url}#chunk-{idx}\", vec, {\n",
        "            'url': page_url,\n",
        "            'title': (page.metadata.title if page.metadata else None),\n",
        "            'chunk_index': idx,\n",
        "            'content': chunk,\n",
        "        }))\n",
        "\n",
        "if site_records:\n",
        "    col.upsert(records=site_records)\n",
        "    print(f\"Upserted crawl chunks: {len(site_records)}\")\n",
        "else:\n",
        "    print('No crawl content found')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Q: What does the scrape endpoint do?\n",
            "Top 3 matches:\n",
            "('https://docs.supacrawler.com/api/scrape#chunk-0', {'url': 'https://docs.supacrawler.com/api/scrape', 'title': 'Scrape - Supacrawler API Reference', 'content': '# scrape # # [ quick example ] ( \\\\ # qu ... (1112 characters truncated) ... eintegerdescription crawl depth used ( only for links format ). * * * get / v1 / scrape # # [ scrape a webpage ] ( \\\\ # scrape - a', 'chunk_index': 0}) n/a None\n",
            "('https://docs.supacrawler.com/api/scrape#chunk-3', {'url': 'https://docs.supacrawler.com/api/scrape', 'title': 'Scrape - Supacrawler API Reference', 'content': '##l ` typestringdescription the url of t ... (1219 characters truncated) ...  - name ` 400 bad request ` description invalid url or missing required parameters. - name ` 401 unauthorized ` descriptioninvalid', 'chunk_index': 3}) n/a None\n",
            "('https://docs.supacrawler.com/quickstart#chunk-1', {'url': 'https://docs.supacrawler.com/quickstart', 'title': 'Quickstart - Supacrawler API Reference', 'content': \"how to send a get request to the scr ... (1045 characters truncated) ... erstand error handling ] ( / errors ) for robust integrations # # [ common use cases ] ( \\\\ # common - use - cases ) # # # content\", 'chunk_index': 1}) n/a None\n"
          ]
        }
      ],
      "source": [
        "# 3) Ask: \"What are the endpoints?\"\n",
        "from vecs import IndexMeasure\n",
        "try:\n",
        "    col.create_index(measure=IndexMeasure.cosine_distance)\n",
        "except Exception:\n",
        "    pass\n",
        "q = \"What does the scrape endpoint do?\"\n",
        "qv = embed_text(q)\n",
        "results = col.query(data=qv, limit=3, include_metadata=True)\n",
        "print(f\"\\nQ: {q}\\nTop {len(results)} matches:\")\n",
        "for rec in results:\n",
        "    rec_id, score, metadata = normalize_vecs_result(rec)\n",
        "    title = metadata.get('title') if isinstance(metadata, dict) else None\n",
        "    snippet = (metadata.get('content') or '')[:240] if isinstance(metadata, dict) else ''\n",
        "    print(rec_id, (round(score, 4) if isinstance(score, (int, float)) else 'n/a'), title)\n",
        "    if snippet:\n",
        "        print('  ', snippet.replace('\\n', ' ') + ('...' if len(snippet) == 240 else ''))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
